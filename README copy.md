# Imitation Learning from Suboptimal Demonstrations via Meta-Learning an Action Ranker

## Dependencies

### Hyper-parameters

To train an expert or an Imitation Learning algorithm that reproduces the results, you can use the hyper-parameters we provided in ` ./hyperparam.yaml`.

### Train experts

To train experts, we provide algorithms including Proximal Policy Optimization (PPO) [[5]](#references) and Soft Actor-Critic (SAC) [[6]](#references). Also, we provide some well-trained experts' weights in the `./weights` folder. The file name means the number of steps the expert is trained. 

For example, you can use the following line to train an expert:

```bash
python train_expert.py --env-id Ant-v2 --algo ppo --num-steps 10000000 --eval-interval 100000 --rollout 10000 --seed 0
```
python train_expert.py --env-id Walker2d-v2 --algo ppo --num-steps 20000000 --eval-interval 100000 --rollout 10000 --seed 0
python train_expert.py --env-id Hopper-v2 --algo ppo --num-steps 30000000 --eval-interval 100000 --rollout 10000 --seed 0

python train_expert.py --env-id Humanoid-v2 --algo sac --num-steps 20000000 --eval-interval 100000 --rollout 1000000 --seed 0


After training, the logs will be saved in folder `./logs/<env_id>/<algo>/seed<seed>-<training time>/`. The folder contains `summary` to track the process of training, and `model` to save the models.

### Collect demonstrations

First, create a folder for demonstrations:

```bash
mkdir buffers
```

You need to collect demonstraions using trained experts' weights.  `--algo` specifies the expert's algorithm, For example, if you train an expert with ppo, you need to use `--algo ppo` while collecting demonstrations. The experts' weights we provide are trained by PPO for Ant-v2, and SAC for Reacher-v2. `--std` specifies the standard deviation of the gaussian noise add to the action, and `--p-rand` specifies the probability the expert acts randomly. We set `std` to $0.01$ not to collect too similar trajectories.

Use the following line to collect expert demonstrations. 
In our experiments, we set the size of the buffers to contain 1 trajectory in expert demonstration, So the buffer size is 1000.
we set the size of the suboptimal buffers to contain 500 trajectories in generated by 5 different policies. So the buffer size is 100000 each policy. 

```bash
python collect_demo.py --weight "./weights/Ant-v2/10000000.pkl" --env-id Ant-v2 --buffer-size 100000 --algo ppo --std 0.01 --p-rand 0 --seed 0
python collect_demo.py --weight "./weights/Humanoid-v2/14100000.pth" --env-id Humanoid-v2 --buffer-size 100000 --algo sac --std 0.01 --p-rand 0.0 --seed 0


```

After collecting, the demonstrations will be saved in the`./buffers/Raw/<env_id>` folder. The file names of the demonstrations indicate the sizes and the mean rewards. 

### Mix Demonstrations

You can create a mixture of demonstrations using the collected demonstrations in the previous step. Use the following command to mix the demonstrations.
Note don't mix the expert with others if you use a same random seed, the first trajectory when you use expert policy collect demonstrations is this.
python mix_demo.py --env-id Humanoid-v2 --folder "./buffers/Raw100/Humanoid-v2"
```

After mixing, the mixed demonstrations will be saved in the `./buffers/<env_id>` folder. The file names of the demonstrations indicate the sizes and the mean rewards of its mixed parts. 



### Train Imitation Learning

You can train IL using the following line:


```bash
python train_imitation.py --algo ilamr --env-id Ant-v2 --buffer_exp ".//home/fanjiangdong/workspace/The_second_idea/buffers_100/Ant-v2/size1000_reward4791.9.pth" --buffer_union "./home/fanjiangdong/workspace/The_second_idea/buffers_100/Ant-v2/size500000_reward_2060.1_2862.87_3641.36_4755.81_793.44.pth" --seed 2026
python train_imitation.py --algo ilamr --env-id Hopper-v2 --buffer_exp "./buffers_100/Hopper-v2/size1000_reward3637.46.pth" --buffer_union "./buffers_100/Hopper-v2/size500000_reward_1918.29_2789.23_3244.98_3635.33_560.29.pth" --seed 2024
python train_imitation.py --algo ilamr --env-id Humanoid-v2 --buffer_exp "./buffers_100/Humanoid-v2/size1000_reward7006.71.pth" --buffer_union "./buffers_100/Humanoid-v2/size500000_reward_2843.49_4327.42_5328.98_5845.06_6845.13.pth" --seed 2022
python train_imitation.py --algo ilamr --env-id Walker2d-v2 --buffer_exp "./buffers_100/Walker2d-v2/size1000_reward4299.35.pth" --buffer_union "./buffers_100/Walker2d-v2/size500000_reward_1418.52_2900.46_3065.32_3703.35_772.6.pth" --seed 2024

### Evaluation

If you want to evaluate an agent, use the following line:

```bash
python eval_policy.py --weight "./logs/Ant-v2/.../actor.pth" --env-id Hopper-v2 --algo ilmar --episodes 5 --render --seed 0 --delay 0.02
```

### Tracking Training Process

We use [TensorBoard](https://www.tensorflow.org/tensorboard) to track the training process. Use the following line to see how everything goes on during the training:

```bash
tensorboard --logdir='<path-to-summary>'
```

### Make Gifs

We provide the following command to make gifs of trained agents:

```bash
python make_gif.py --env-id Ant-v2 --weight "./logs/Ant-v2/.../actor.pth" --algo ilmar --episodes 1

```

This will generate `./figs/<env_id>/actor_0.gif`, which shows the performance of the agent.

## Results

















rl_plotter --save --show --avg_group --shaded_err --no_legend_group_num --legend_outside --smooth 10 



            ant     Hopper  Humanoid    Walker2d
BC          76.61   52.29   62.51       76.13       66.885
DemoDICE    76.82   59.53   54.77       78.10       67.305
ISWBC       84.28   49.06   21.78       75.83       57.7375
ILMAR       93.07   61.02   73.07       67.40       73.64